# 近端策略优化算法（PPO）

基于人类反馈的强化学习主要分为**奖励模型训练**和**近端策略优化**两个步骤。

- 奖励模型通过由人类反馈标注的偏好数据来学习人类的偏好，判断模型回复的有用性以及保证内容的无害性。奖励模型模拟了人类的偏好信息，能够不断地为模型的训练提供奖励信号。

- 在获得奖励模型后，需要借助强化学习算法对语言模型继续进行微调。近端策略优化算法（PPO）就是其中一种，它是由openAI提出的一种高效的强化学习算法。

## 近端策略优化

近端策略优化涉及到四个模型：

1. **策略模型（Policy Model）**：生成模型回复，它是被调整的模型。
2. **奖励模型（Reward Model）**：输出奖励分数来评价回复质量的好坏。
3. **评论模型（Critic Model）**：预测回复的好坏，可以在训练过程中实时调整模型，选择对未来累积收益最大的行为。
4. **参考模型（Reference Model）**：提供一个SFT模型的备份， 使模型不会出现过于极端的变化。

近端策略优化的流程：

1. **环境采样**：策略模型基于给定输入生成一系列回复，奖励模型则对这些回复进行打分获得奖励。
2. **优势估计**：利用评论模型预测生成回复的未来累积奖励，并借助**广义优势估计（Generalized Advantage Estimation，GAE）**算法来估计优势函数，能够有助于更准确地评估每次行动的好处。
3. **优化调整**：使用优势函数来优化和调整策略模型，同时利用参考模型确保更新的策略不会有太大的变化，从而维持模型的稳定性。

## 奖励模型

奖励模型通常也采用基于Transformer架构的预训练语言模型。在奖励模型中，移除最后一个非嵌入层，并在最终的Transformer层上叠加了一个额外的线性层。无论输入的是何种文本，奖励模型都能为文本序列中的最后一个标记分配一个标量奖励值，样本质量越高，奖励值越大。

训练奖励模型通常需要使用由相同输入生成的两个不同输出之间的配对比较数据集。在这个数据集中，每一对包括一个首选样本和一个非首选样本，利用这些数据来建模奖励模型的训练损失。每一对样本的模型损失可以定义为：

$$\mathcal{L}\left(\psi\right)=\log\sigma\left(r\left(x,y_{w}\right)-r\left(x,y_{l}\right)\right)$$

![image-20251015233346486](assets/image-20251015233346486.png)

