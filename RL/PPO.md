# 近端策略优化算法（PPO）

基于人类反馈的强化学习主要分为**奖励模型训练**和**近端策略优化**两个步骤。

- 奖励模型通过由人类反馈标注的偏好数据来学习人类的偏好，判断模型回复的有用性以及保证内容的无害性。奖励模型模拟了人类的偏好信息，能够不断地为模型的训练提供奖励信号。

- 在获得奖励模型后，需要借助强化学习算法对语言模型继续进行微调。近端策略优化算法（PPO）就是其中一种，它是由openAI提出的一种高效的强化学习算法。

## 近端策略优化

近端策略优化涉及到四个模型：

1. **策略模型（Policy Model）**：生成模型回复，它是被调整的模型。
2. **奖励模型（Reward Model）**：输出奖励分数来评价回复质量的好坏。
3. **评论模型（Critic Model）**：预测回复的好坏，可以在训练过程中实时调整模型，选择对未来累积收益最大的行为。
4. **参考模型（Reference Model）**：提供一个SFT模型的备份， 使模型不会出现过于极端的变化。

近端策略优化的流程：

1. **环境采样**：策略模型基于给定输入生成一系列回复，奖励模型则对这些回复进行打分获得奖励。
2. **优势估计**：利用评论模型预测生成回复的未来累积奖励，并借助**广义优势估计（Generalized Advantage Estimation，GAE）**算法来估计优势函数，能够有助于更准确地评估每次行动的好处。
3. **优化调整**：使用优势函数来优化和调整策略模型，同时利用参考模型确保更新的策略不会有太大的变化，从而维持模型的稳定性。