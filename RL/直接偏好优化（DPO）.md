# 直接偏好优化（DPO）

虽然大规模无监督语言模型（LLMs）能够学习广泛的世界知识和一定的推理能力，但是由于其训练过程完全无监督，要实现对其行为的精确控制是非常困难的。现有的方法通过收集模型生成的相对质量的人类标注，微调LLM以符合这些偏好，通常采用从人类反馈中进行强化学习（RLHF）。然而，RLHF是一种复杂且通常不稳定的过程，首先需要拟合反馈人类偏好的奖励模型，然后使用强化学习来微调LLM以最大化这一估计的奖励，同时避免与原始模型偏离过远。直接偏好优化（DPO）算法是一种新的强化学习方法，它仅需通过一个**简单的分类损失**来解决标准的RLHF方法的问题。**其具有稳定、高效且计算负担轻的特点**，无需在微调过程种从LLM采样或进行大量超参数调整。它通过一个简单的二元交叉熵目标来精确优化，从而极大的简化了偏好学习流程。

![img](assets/v2-a2cf5f95d8fa7c50a677882455df20ef_1440w.jpg)

DPO可以直接依据策略来定义偏好损失。当存在一个关于模型响应的人类偏好数据集时，DPO能够在训练过程中，使用简单的二元交叉熵目标来对策略进行优化，而无需明确地区学习奖励函数或者从策略中进行采样。

$$\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})=-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\left[\log\sigma\left(\beta\log\frac{\pi_{\theta}(y_{w}\mid x)}{\pi_{\mathrm{ref}}(y_{w}\mid x)}-\beta\log\frac{\pi_{\theta}(y_{l}\mid x)}{\pi_{\mathrm{ref}}(y_{l}\mid x)}\right)\right]$$

DPO算法的优化目标非常简单，利用了从奖励函数到最优策略的解析映射，允许直接使用人类偏好数据进行简化的优化过程。

该目标增加了对偏好数据$y_w$的可能性，并减少非偏好数据$y_l$的可能性。

$$\hat{r}_\theta(x,y)=\beta\log\frac{\pi_\theta(y|x)}{\pi_\mathrm{ref}(y|x)}$$

这个公式其实有点像变种的奖励函数，通过$\beta$参数的权重来调节，$\pi_{\theta}$参数的优化等效与在此变量更改下的奖励模型优化。

DPO的数据集主要由三部分组成：`instruct prompt`、`chosen`、`rejected`。

![img](assets/v2-00c94509773ca9cf683a01584ec11240_1440w.jpg)

从数据集中可以明显的看出，其实就是想让模型拟合我们期望的chosen数据，不要回答rejected类型的数据。这种思想很像对比学习，拉开正负样本差距，让模型学习指定输出正确的回答。

