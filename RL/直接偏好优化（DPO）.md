# 直接偏好优化（DPO）

虽然大规模无监督语言模型（LLMs）能够学习广泛的世界知识和一定的推理能力，但是由于其训练过程完全无监督，要实现对其行为的精确控制是非常困难的。现有的方法通过收集模型生成的相对质量的人类标注，微调LLM以符合这些偏好，通常采用从人类反馈中进行强化学习（RLHF）。然而，RLHF是一种复杂且通常不稳定的过程，首先需要拟合反馈人类偏好的奖励模型，然后使用强化学习来微调LLM以最大化这一估计的奖励，同时避免与原始模型偏离过远。直接偏好优化（DPO）算法是一种新的强化学习方法，它仅需通过一个**简单的分类损失**来解决标准的RLHF方法的问题。**其具有稳定、高效且计算负担轻的特点**，无需在微调过程种从LLM采样或进行大量超参数调整。

